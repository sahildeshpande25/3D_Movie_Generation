# -*- coding: utf-8 -*-
"""2D_to_3D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M3g4wCdLGKqDnXSIHLH4UcbZM_pNJY_m
"""

import os
import numpy as np
from PIL import Image
import glob

import cv2
import torch
import torchvision   
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import torch.utils.data as data

class Dataset(data.Dataset):

    def get_image_pairs(self, folder_path):
        os.chdir(os.path.join(folder_path, 'left'))
        left = glob.glob('*.jpg')
        os.chdir(os.path.join('..', 'right'))
        right = glob.glob('*.jpg')
        os.chdir(os.path.join('..', '..', '..', '..'))
        left_end = '_left.jpg'
        right_end = '_right.jpg'
        left = [l[:-len(left_end)] for l in left]
        right = [r[:-len(right_end)] for r in right]

        pairs = set(left).intersection(set(right))

        return list(pairs)

    def __init__(self, folder_path, transform=None):
        super(Dataset, self).__init__()
        self.pairs = self.get_image_pairs(folder_path)
        self.left_files = [os.path.join(folder_path, 'left', p) +'_left.jpg' for p in self.pairs]
        self.right_files = [os.path.join(folder_path, 'right', p) +'_right.jpg' for p in self.pairs]

    def __getitem__(self, index):

        left_img_path = self.left_files[index]
        right_img_path = self.right_files[index]

        #print(left_img_path, right_img_path)

        left = cv2.resize(cv2.imread(left_img_path), (384, 160)).astype(np.float32)
        right = cv2.resize(cv2.imread(right_img_path), (384, 160)).astype(np.float32)
        left = (torch.from_numpy(left).permute(2, 0, 1))
        right = (torch.from_numpy(right).permute(2, 0, 1))

        return left, right

    def __len__(self):
        return len(self.left_files)

folder_path = os.path.join('dataset', 'Holopix50k', 'train')
train_dataset = Dataset(folder_path)
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)

folder_path = os.path.join('dataset', 'Holopix50k', 'val')
val_dataset = Dataset(folder_path)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)

class Left2Right(nn.Module):
    def __init__(self):
        super(Left2Right, self).__init__()
        
        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)
        self.relu1_1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)
        self.relu2_1 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)
        self.relu3_1 = nn.ReLU()
        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)
        self.relu3_2 = nn.ReLU()
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)
        self.relu4_1 = nn.ReLU()
        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)
        self.relu4_2 = nn.ReLU()
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)
        self.relu5_1 = nn.ReLU()
        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)
        self.relu5_2 = nn.ReLU()
        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.flatten = nn.Flatten()
        self.fc6 = nn.Linear(30720, 512)  
        self.relu6 = nn.ReLU()
        self.drop6 = nn.Dropout(p=0.5)

        self.fc7 = nn.Linear(512, 512)  
        self.relu7 = nn.ReLU()
        self.drop7 = nn.Dropout(p=0.5)

        self.fc8 = nn.Linear(512, 33*12*5)

        self.bn_pool4 = nn.BatchNorm2d(512)
        self.pred4 = nn.Conv2d(in_channels=512 ,out_channels=33, kernel_size=3 ,padding=1)
        self.bn_pool3 = nn.BatchNorm2d(256)
        self.pred3 = nn.Conv2d(in_channels=256 ,out_channels=33, kernel_size=3 ,padding=1)
        self.bn_pool2 = nn.BatchNorm2d(128)
        self.pred2 = nn.Conv2d(in_channels=128 ,out_channels=33, kernel_size=3 ,padding=1)
        self.bn_pool1 = nn.BatchNorm2d(64)
        self.pred1 = nn.Conv2d(in_channels=64 ,out_channels=33, kernel_size=3 ,padding=1)
        
        workspace = 0
        scale = 1

        self.relu = nn.ReLU()

        self.deconv_pred1 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=1, padding=0, stride=1)
        scale *= 2

        self.deconv_pred2 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)
        scale *= 2

        self.deconv_pred3 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)
        scale *= 2

        self.deconv_pred4 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)
        scale *= 2

        self.deconv_pred5 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)
        self.relu1 =  nn.ReLU()
        scale = 2
        self.deconv_predup = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)
        self.relu2 = nn.ReLU()
        self.convolution0 = nn.Conv2d(in_channels=33 ,out_channels=33, kernel_size=3 ,padding=1)

        self.softmax = nn.Softmax()

    def forward(self, x, evalMode=False):
        
        out = x
        
        out = self.relu1_1(self.conv1_1(out))
        pool1 = self.pool1(out)

        out = self.relu2_1(self.conv2_1(pool1))
        pool2 = self.pool2(out)

        out = self.relu3_1(self.conv3_1(pool2))
        out = self.relu3_2(self.conv3_2(out))
        pool3 = self.pool3(out)

        out = self.relu4_1(self.conv4_1(pool3))
        out = self.relu4_2(self.conv4_2(out))
        pool4 = self.pool4(out)

        out = self.relu5_1(self.conv5_1(pool4))
        out = self.relu5_2(self.conv5_2(out))
        pool5 = self.pool5(out)

        out = self.flatten(pool5)

        out = self.drop6(self.relu6(self.fc6(out)))
        out = self.drop7(self.relu7(self.fc7(out)))

        out = self.fc8(out)

        pred5 = torch.reshape(out, (out.shape[0], 33, 5, 12))

        pred4 = self.bn_pool4(pool4)
        pred4 = self.pred4(pred4)
        pred3 = self.bn_pool3(pool3)
        pred3 = self.pred3(pred3)
        pred2 = self.bn_pool2(pool2)
        pred2 = self.pred2(pred2)
        pred1 = self.bn_pool1(pool1)
        pred1 = self.pred1(pred1)

        pred1 = self.deconv_pred1(self.relu(pred1))
        pred2 = self.deconv_pred2(self.relu(pred2))
        pred3 = self.deconv_pred3(self.relu(pred3))
        pred4 = self.deconv_pred4(self.relu(pred4))
        pred5 = self.deconv_pred5(self.relu(pred5))

        pred = pred1 + pred2 + pred3 + pred4 + pred5
        pred = self.relu(pred)

        pred = self.convolution0(self.relu(self.deconv_predup(pred)))

        mask = self.softmax(pred)

        return mask

def selectionLayer(masks, left_image, left_shift=16):

    p2d = (left_shift, left_shift, 0, 0)
    padded_img = F.pad(left_image, p2d, 'constant')

    depth = masks.shape[1]
    width = left_image.shape[3]
    layers = []
    for d in range(depth-1,-1,-1):
        layers.append(padded_img[:,:,:,d:d+width])
    layers = torch.stack(layers, axis=1)
    disparity_image =  layers * masks.unsqueeze(2)

    return torch.sum(disparity_image, axis=1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

epochs_completed = 0

model = Left2Right()
model_path = 'models/pretrained_weights_{}.pt'
# checkpoint = torch.load(model_path.format(epochs_completed-1))
# model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(device)

def train(model, data_loader, val_loader, n_epochs, criterion, scheduler):

    for epoch in range(n_epochs):
        model.train()
        avg_loss = 0.0
        num_correct = 0.0
        total = len(data_loader)

        for batch_num, (left_img, right_img) in enumerate(data_loader):

            if batch_num%100==0:
                print(batch_num)
            left_img, right_img = left_img.to(device), right_img.to(device)
            
            optimizer.zero_grad()

            outputs = model(left_img)
            outputs = selectionLayer(outputs, left_img)

            loss = criterion(outputs, right_img)
            loss.backward()
            optimizer.step()
            
            avg_loss += loss.item()
            
            torch.cuda.empty_cache()
            del left_img
            del right_img
            del loss

        scheduler.step(avg_loss)
        print('Epoch: {}\tAvg-Loss: {:.5}'.format(epoch+1, avg_loss/total))

        torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        }, model_path.format(epochs_completed + epoch))

        val(model, val_loader, criterion)

def val(model, data_loader, criterion):

    model.eval()
    total = len(data_loader)

    avg_loss = 0.0
    num_correct = 0.0

    for batch_num, (left_img, right_img) in enumerate(data_loader):

        left_img, right_img = left_img.to(device), right_img.to(device)
        outputs = model(left_img)
        outputs = selectionLayer(outputs, left_img)
        loss = criterion(outputs, right_img)
        avg_loss += loss.item()

    print('Avg-Val-Loss: {:.5f}'.format(avg_loss/total))

criterion = nn.L1Loss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
# optimizer.load_state_dict(checkpoint['optimizer_label_state_dict'])
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=0, verbose=True)

if __name__ == '__main__':
    train(model, train_dataloader, val_dataloader, 10, criterion, scheduler)